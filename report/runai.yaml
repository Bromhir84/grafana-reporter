apiVersion: v1
items:
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app: kube-prometheus-stack
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.3.0
      chart: kube-prometheus-stack-72.3.0
      heritage: Helm
      release: kube-prometheus-stack
    name: kube-prometheus-stack-runai.rules
    namespace: kube-prometheus-stack
  spec:
    groups:
    - name: new-gen-rules
      rules:
      - expr: topk by(workload_id) (1,runai_workload_info)
        record: runai_workload_info_unique
      - expr: (label_replace(runai_pod_gpu_info{gpu!=""}, "UUID", "$1", "gpu", "(.*)"))
          * on (UUID) group_left(gpu) (count(runai_dcgm_gpu_utilization) by (UUID,
          gpu))
        record: runai_pod_gpu_info_fixed
      - expr: (topk by(pod_uuid) (1,runai_pod_info)) * on (pod_uuid) group_left(gpu,
          gpu_instance_id) runai_pod_gpu_info_fixed{gpu!=""} or on(pod_uuid) (topk
          by(pod_uuid) (1,runai_pod_info))
        record: runai_pod_info_unique
      - expr: topk by(pod_uuid) (1,runai_scheduler_requested_gpus_per_pod)
        record: runai_scheduler_requested_gpus_per_pod_unique
      - expr: topk by(pod_uuid) (1,runai_scheduler_requested_gpu_memory_mb_per_pod)
        record: runai_scheduler_requested_gpu_memory_mb_per_pod_unique
      - expr: topk by(pod_uuid) (1,runai_scheduler_gpus_limit_per_pod)
        record: runai_scheduler_gpus_limit_per_pod_unique
      - expr: topk by(pod_uuid) (1,runai_scheduler_gpu_memory_mb_limit_per_pod)
        record: runai_scheduler_gpu_memory_mb_limit_per_pod_unique
      - expr: topk by(pod_uuid) (1,runai_scheduler_allocated_gpus_per_pod)
        record: runai_scheduler_allocated_gpus_per_pod_unique
      - expr: topk by(pod_uuid) (1,runai_scheduler_allocated_gpu_memory_mb_per_pod)
        record: runai_scheduler_allocated_gpu_memory_mb_per_pod_unique
      - expr: topk by(pod_uuid) (1,runai_scheduler_allocated_millicpus_per_pod)
        record: runai_scheduler_allocated_millicpus_per_pod_unique
      - expr: topk by(pod_uuid) (1,runai_scheduler_allocated_memory_bytes_per_pod)
        record: runai_scheduler_allocated_memory_bytes_per_pod_unique
      - expr: group(runai_workload_info_unique * on (workload_id) group_left(is_gpu)
          (label_replace((sum(runai_scheduler_requested_gpus_per_pod_unique) by (workload_id))
          > 0, "is_gpu", "true", "pod_name", "(.*)") or on(workload_id) label_replace((sum(runai_scheduler_requested_gpus_per_pod_unique)
          by (workload_id)) == 0, "is_gpu", "false", "pod_name", "(.*)"))) by (nodepool,
          project, department, queue_name, workload_name, workload_type, workload_id,
          user, is_gpu, phase, detailed_status, status)
        record: runai_workload_info_enriched
      - expr: (runai_pod_info_unique * on (node) group_left(modelName) runai_node_info_enriched
          or on(pod_uuid) runai_pod_info_unique) * on(workload_id) group_left(workload_name,
          workload_type, user) runai_workload_info_enriched
        record: runai_pod_info_enriched
      - expr: (runai_pod_info_enriched * on(workload_id, nodepool) group_left() topk(1,
          group(runai_pod_info_enriched) by(nodepool, workload_id)) by(workload_id))
        record: runai_pod_info_enriched_nodepool_unique
      - expr: sum(runai_scheduler_requested_gpus_per_pod_unique) by(workload_id) *
          on(workload_id) group_right() runai_workload_info_enriched{detailed_status="Pending"}
        record: runai_pending_requested_gpu_count_per_workload
      - expr: sum by(node, nodepool, project, department, queue_name, pod_name, pod_uuid,
          pod_namespace, workload_name, workload_type, workload_id, gpu, modelName,
          service) (runai_scheduler_allocated_gpu_memory_mb_per_pod_unique) * on (pod_uuid)
          group_right() runai_pod_info_enriched_nodepool_unique
        record: runai_allocated_gpu_memory_mb_per_pod
      - expr: sum(runai_allocated_gpu_memory_mb_per_pod) by (nodepool, project, department,
          queue_name, workload_name, workload_type, workload_id, user) * on(workload_id)
          group_left(nodepool, project, department, queue_name, workload_name, workload_type,
          user, is_gpu, phase, detailed_status, status) runai_workload_info_enriched
        record: runai_allocated_gpu_memory_mb_per_workload
      - expr: group((runai_node_nodepool==1 or on(node) kube_node_info) unless on(nodepool)
          runai_node_nodepool{nodepool="runai-excluded-nodes"}==1) by(node, nodepool)
        record: runai_node_nodepool_excluded
      - expr: sum by(node, nodepool, modelName, ready, schedulable) (kube_node_info
          * on(node) group_left(nodepool) runai_node_nodepool_excluded) * on(node)
          group_left(modelName) (group(runai_gpu_memory_total_mebibytes_per_gpu) by(node,
          modelName) or on(node) kube_node_info) * on(node) group_left(ready) (group(label_replace(kube_node_status_condition{condition="Ready"},
          "ready", "$1", "status", "(.*)")==1) by(node, ready)) * on(node) group_left(schedulable)
          (group(label_replace(kube_node_spec_unschedulable == 0, "schedulable", "true",
          "schedulable", "(.*)") or on(node) label_replace(kube_node_spec_unschedulable
          == 1, "schedulable", "false", "schedulable", "(.*)")) by(node,schedulable))
        record: runai_node_info_enriched
      - expr: (sum(runai_requested_mig_devices) by (queue_name,workload_name,workload_type,mig_device_type,project))
          * on(workload_name,project) group_left(workload_id) runai_workload_info_enriched
        record: runai_requested_mig_devices_per_workload
      - expr: sum by (node, nodepool, modelName, ready, schedulable) ((count by (node)
          (runai_gpu_memory_total_mebibytes_per_gpu) or on(node) kube_node_info*0)
          * on(node) group_right() runai_node_info_enriched)
        record: runai_gpu_count_per_node
      - expr: sum(runai_gpu_count_per_node) by (nodepool)
        record: runai_total_gpus_per_nodepool
      - expr: sum(runai_gpu_count_per_node) by (nodepool)
        record: runai_gpu_count_per_nodepool
      - expr: sum(runai_gpu_count_per_node{ready="true",schedulable="true"}) by (nodepool)
        record: runai_ready_gpu_count_per_nodepool
      - expr: sum(runai_gpu_count_per_nodepool)
        record: runai_gpu_count_per_cluster
      - expr: sum(runai_ready_gpu_count_per_nodepool)
        record: runai_ready_gpu_count_per_cluster
      - expr: count(runai_gpu_count_per_node>0) by (nodepool)
        record: runai_gpu_nodes_count_per_nodepool
      - expr: count(runai_gpu_count_per_node{ready="true",schedulable="true"}>0) by
          (nodepool)
        record: runai_ready_gpu_nodes_count_per_nodepool
      - expr: sum(runai_gpu_nodes_count_per_nodepool)
        record: runai_gpu_nodes_count_per_cluster
      - expr: sum(runai_ready_gpu_nodes_count_per_nodepool)
        record: runai_ready_gpu_nodes_count_per_cluster
      - expr: sum(runai_pending_requested_gpu_count_per_workload) by (queue_name,
          project, department, nodepool)
        record: runai_pending_requested_gpu_count_per_queue
      - expr: sum(runai_pending_requested_gpu_count_per_queue) by (nodepool)
        record: runai_pending_requested_gpu_count_per_nodepool
      - expr: sum(runai_pending_requested_gpu_count_per_nodepool)
        record: runai_pending_requested_gpu_count_per_cluster
      - expr: sum by(node, nodepool, project, department, queue_name, pod_name, pod_uuid,
          pod_namespace, workload_name, workload_type, workload_id, gpu, modelName,
          service) (runai_scheduler_allocated_gpus_per_pod_unique) * on (pod_uuid)
          group_right() runai_pod_info_enriched_nodepool_unique
        record: runai_allocated_gpu_count_per_pod
      - expr: sum(runai_allocated_gpu_count_per_pod) by (nodepool, project, department,
          queue_name, workload_name, workload_type, workload_id, user) * on(workload_id)
          group_left(nodepool, project, department, queue_name, workload_name, workload_type,
          user, is_gpu, phase, detailed_status, status) runai_workload_info_enriched
        record: runai_allocated_gpu_count_per_workload
      - expr: sum(runai_allocated_gpu_count_per_workload>0) by (workload_type, queue_name,
          project, department, nodepool)
        record: runai_allocated_gpu_count_per_queue_per_workload_type
      - expr: sum(runai_allocated_gpu_count_per_workload) by (queue_name, project,
          department, nodepool) or on(queue_name) group(runai_queue_info) by(queue_name,
          project, department, nodepool)*0
        record: runai_allocated_gpu_count_per_queue
      - expr: sum(runai_allocated_gpu_count_per_pod) by (project, department, nodepool)
          or on(project) (group(label_replace(runai_queue_info{project!=""}, "department",
          "$1", "department_name", "(.*)")) by(project, department)*0)
        record: runai_allocated_gpu_count_per_project_nodepool
      - expr: sum(runai_allocated_gpu_count_per_queue_per_workload_type) by (workload_type,
          project, department)
        record: runai_allocated_gpu_count_per_project_per_workload_type
      - expr: sum(runai_allocated_gpu_count_per_queue) by (project, department)
        record: runai_allocated_gpu_count_per_project
      - expr: sum(runai_allocated_gpu_count_per_queue) by (department)
        record: runai_allocated_gpu_count_per_department
      - expr: sum(((sum(runai_allocated_gpu_count_per_pod{gpu!=""}) by(gpu, node)
          or count(runai_gpu_memory_used_mebibytes_per_pod_per_gpu) by(node, gpu))
          * on(gpu, node) group_right() group(runai_gpu_memory_total_mebibytes_per_gpu)
          by(UUID, gpu, node, nodepool, modelName)) or on(UUID) runai_gpu_memory_total_mebibytes_per_gpu*0)
          by(UUID, gpu, node, nodepool, modelName)
        record: runai_allocated_gpu_count_per_gpu
      - expr: ((sum by (node) (runai_allocated_gpu_count_per_pod>0)) * on (node) group_right()
          runai_node_info_enriched) or on(node) runai_node_info_enriched*0
        record: runai_allocated_gpu_count_per_node
      - expr: sum(runai_allocated_gpu_count_per_node) by (nodepool)
        record: runai_allocated_gpu_count_per_nodepool
      - expr: sum(runai_allocated_gpu_count_per_nodepool)
        record: runai_allocated_gpu_count_per_cluster
      - expr: count(runai_allocated_gpu_count_per_gpu == 0) by (clusterId, node, nodepool)
          or on (clusterId, node, nodepool) 0*group(runai_allocated_gpu_count_per_node
          > 0) by (clusterId, node, nodepool)
        record: runai_free_gpu_count_per_node
      - expr: 100*(runai_allocated_gpu_count_per_node / runai_gpu_count_per_node>0)
        record: runai_gpu_allocation_percentage_per_node
      - expr: 100*(runai_allocated_gpu_count_per_nodepool / runai_gpu_count_per_nodepool>0)
        record: runai_gpu_allocation_percentage_per_nodepool
      - expr: 100*(runai_allocated_gpu_count_per_cluster / runai_gpu_count_per_cluster>0)
        record: runai_gpu_allocation_percentage_per_cluster
      - expr: sum by(queue_name, project, department, nodepool) (runai_queue_deserved_gpus)
        record: runai_deserved_gpu_count_per_queue
      - expr: min(runai_deserved_gpu_count_per_queue{project!=""}<0) by(project, department)
          or on(project) sum(runai_deserved_gpu_count_per_queue{project!=""}>=0) by(project,
          department)
        record: runai_deserved_gpu_count_per_project
      - expr: min(runai_deserved_gpu_count_per_queue{project=""}<0) by(department)
          or on(department) sum(runai_deserved_gpu_count_per_queue{project=""}>=0)
          by(department)
        record: runai_deserved_gpu_count_per_department
      - expr: sum by(UUID, gpu, node, nodepool, modelName) (label_replace(runai_gpu_oomkill_burst_count,
          "pod_ip", "$1", "instance", "(.*):(.*)") * on (pod_ip) group_left(node)
          topk(1, kube_pod_info{pod_ip!=""}) by (pod_ip) * on (node) group_left(nodepool,
          modelName) runai_node_info_enriched)
        record: runai_gpu_oomkill_burst_count_per_gpu
      - expr: sum by(node, nodepool, modelName, ready, schedulable) (runai_gpu_oomkill_burst_count_per_gpu)
        record: runai_gpu_oomkill_burst_count_per_node
      - expr: sum by(nodepool) (runai_gpu_oomkill_burst_count_per_node)
        record: runai_gpu_oomkill_burst_count_per_nodepool
      - expr: sum by(UUID, gpu, node, nodepool, modelName) (label_replace(runai_gpu_oomkill_idle_count,
          "pod_ip", "$1", "instance", "(.*):(.*)") * on (pod_ip) group_left(node)
          topk(1, kube_pod_info{pod_ip!=""}) by (pod_ip) * on (node) group_left(nodepool,
          modelName) runai_node_info_enriched)
        record: runai_gpu_oomkill_idle_count_per_gpu
      - expr: sum by(node, nodepool, modelName, ready, schedulable) (runai_gpu_oomkill_idle_count_per_gpu)
        record: runai_gpu_oomkill_idle_count_per_node
      - expr: sum by(nodepool) (runai_gpu_oomkill_idle_count_per_node)
        record: runai_gpu_oomkill_idle_count_per_nodepool
      - expr: sum by(UUID, gpu, node, nodepool, modelName) (label_replace(runai_gpu_oomkill_priority_count,
          "pod_ip", "$1", "instance", "(.*):(.*)") * on (pod_ip) group_left(node)
          topk(1, kube_pod_info{pod_ip!=""}) by (pod_ip) * on (node) group_left(nodepool,
          modelName) runai_node_info_enriched)
        record: runai_gpu_oomkill_priority_count_per_gpu
      - expr: sum by(node, nodepool, modelName, ready, schedulable) (runai_gpu_oomkill_priority_count_per_gpu)
        record: runai_gpu_oomkill_priority_count_per_node
      - expr: sum by(nodepool) (runai_gpu_oomkill_priority_count_per_node)
        record: runai_gpu_oomkill_priority_count_per_nodepool
      - expr: sum by(UUID, gpu, node, nodepool, modelName) (label_replace(runai_gpu_oomkill_swap_out_of_ram_count,
          "pod_ip", "$1", "instance", "(.*):(.*)") * on (pod_ip) group_left(node)
          topk(1, kube_pod_info{pod_ip!=""}) by (pod_ip) * on (node) group_left(nodepool,
          modelName) runai_node_info_enriched)
        record: runai_gpu_oomkill_swap_out_of_ram_count_per_gpu
      - expr: sum by(node, nodepool, modelName, ready, schedulable) (runai_gpu_oomkill_swap_out_of_ram_count_per_gpu)
        record: runai_gpu_oomkill_swap_out_of_ram_count_per_node
      - expr: sum by(nodepool) (runai_gpu_oomkill_swap_out_of_ram_count_per_node)
        record: runai_gpu_oomkill_swap_out_of_ram_count_per_nodepool
      - expr: runai_pod_info_enriched_nodepool_unique{gpu!="", gpu_instance_id=""}
          * on (pod_uuid) group_right(node, nodepool, project, department, queue_name,
          pod_name, pod_namespace, workload_name, workload_type, workload_id, modelName)
          runai_pod_gpu_swap_ram_used_bytes or on(pod_uuid) runai_pod_info_enriched_nodepool_unique{gpu!="",
          gpu_instance_id=""}*0
        record: runai_swap_memory_bytes_per_pod_per_gpu
      - expr: "label_replace(label_replace(label_replace(label_replace(DCGM_FI_DEV_GPU_UTIL
          or ceil(DCGM_FI_PROF_GR_ENGINE_ACTIVE{GPU_I_ID!=\"\"}*100), \"pod_ip\",
          \"$1\", \"instance\", \"(.*):(.*)\"), \"pod_name\", \"$1\", \"exported_pod\",
          \"(.+)\"), \"pod_namespace\", \"$1\", \"exported_namespace\", \"(.+)\"),
          \"gpu_instance_id\",\n\t\"$1\", \"GPU_I_ID\", \"(.+)\") * on (pod_ip) group_left(node)
          kube_pod_info{created_by_name=~\".*dcgm-exporter\", pod_ip!=\"\"} * on(node)
          group_left(nodepool) runai_node_nodepool_excluded"
        record: runai_dcgm_gpu_utilization
      - expr: label_join(label_replace(label_replace(label_replace(label_replace(amd_gpu_use_percent,
          "node", "$1", "node_id", "(.*)"), "modelName", "$1", "productname", "(.*)"),
          "gpu", "$1", "gpu_id", "(.*)"), "pod_uuid", "$1", "pod", "(.*)"), "UUID",
          "-", "node", "gpu") * on(node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_amd_gpu_utilization
      - expr: runai_pod_info_enriched_nodepool_unique{gpu!="", gpu_instance_id=""}
          * on (pod_uuid) group_right(node, nodepool, project, department, queue_name,
          pod_name, pod_namespace, workload_name, workload_type, workload_id, modelName)
          runai_pod_gpu_utilization or on(pod_uuid) runai_pod_info_enriched_nodepool_unique{gpu!="",
          gpu_instance_id=""}*0
        record: runai_gpu_utilization_per_fractional_pod
      - expr: runai_pod_info_enriched_nodepool_unique{gpu!="", gpu_instance_id!=""}
          * on(node, gpu, gpu_instance_id) group_right(pod_name, pod_namespace, workload_name,
          workload_type, workload_id, pod_uuid, project, department, queue_name) runai_dcgm_gpu_utilization
        record: runai_gpu_utilization_per_mig_pod
      - expr: runai_pod_info_enriched_nodepool_unique{gpu="", gpu_instance_id=""}
          * on(pod_name, pod_namespace) group_right(workload_name, workload_type,
          workload_id, pod_uuid, project, department, queue_name) runai_dcgm_gpu_utilization
          or on (pod_uuid) runai_pod_info_enriched_nodepool_unique{gpu="", gpu_instance_id=""}
          * on(pod_uuid) group_right(workload_name, workload_type, workload_id, pod_name,
          pod_namespace, project, department, queue_name) runai_amd_gpu_utilization
        record: runai_gpu_utilization_per_whole_pod_per_gpu
      - expr: sum by(node, nodepool, project, department, queue_name, pod_name, pod_uuid,
          pod_namespace, workload_name, workload_type, workload_id, gpu, modelName,
          service) (runai_gpu_utilization_per_fractional_pod or on (pod_uuid) runai_gpu_utilization_per_mig_pod
          or on (pod_uuid) runai_gpu_utilization_per_whole_pod_per_gpu)
        record: runai_gpu_utilization_per_pod_per_gpu
      - expr: avg(runai_gpu_utilization_per_pod_per_gpu) by (node, nodepool, project,
          department, queue_name, pod_name, pod_namespace, workload_name, workload_type,
          workload_id, modelName)
        record: runai_gpu_utilization_per_pod
      - expr: timestamp(runai_gpu_utilization_per_pod > 2) or on(node, nodepool, project,
          department, queue_name, pod_name, pod_namespace, workload_name, workload_type,
          workload_id, modelName) (runai_last_gpu_utilization_time_per_pod * on(node,
          nodepool, project, department, queue_name, pod_name, pod_namespace, workload_name,
          workload_type, workload_id, modelName) group_left() clamp(runai_allocated_gpu_count_per_pod>0,1,1))
          or on(node, nodepool, project, department, queue_name, pod_name, pod_namespace,
          workload_name, workload_type, workload_id, modelName) timestamp(runai_gpu_utilization_per_pod)
        record: runai_last_gpu_utilization_time_per_pod
      - expr: (sum(avg(runai_gpu_utilization_per_pod_per_gpu) by(pod_uuid, workload_id)
          * on(pod_uuid, workload_id) runai_allocated_gpu_count_per_pod) by (workload_id))
          / on(workload_id) (runai_allocated_gpu_count_per_workload>0) * on(workload_id)
          group_right() runai_workload_info_enriched
        record: runai_gpu_utilization_per_workload
      - expr: timestamp(runai_gpu_utilization_per_workload > 2) or on(workload_id)
          (runai_last_gpu_utilization_time_per_workload * on(workload_id) group_left()
          clamp(runai_allocated_gpu_count_per_workload>0,1,1)) or on(workload_id)
          timestamp(runai_gpu_utilization_per_workload)
        record: runai_last_gpu_utilization_time_per_workload
      - expr: time() - runai_last_gpu_utilization_time_per_workload
        record: runai_gpu_idle_seconds_per_workload
      - expr: time() - runai_last_gpu_utilization_time_per_pod
        record: runai_gpu_idle_seconds_per_pod
      - expr: sum(avg(runai_gpu_utilization_per_pod_per_gpu) by(pod_uuid, workload_type,
          queue_name, project, department, nodepool) * on(pod_uuid, workload_type,
          queue_name, project, department, nodepool) runai_allocated_gpu_count_per_pod)
          by (workload_type, queue_name, project, department, nodepool) / on(workload_type,
          queue_name, project, department, nodepool) (runai_allocated_gpu_count_per_queue_per_workload_type>0)
        record: runai_gpu_utilization_per_queue_per_workload_type
      - expr: sum(avg(runai_gpu_utilization_per_pod_per_gpu) by(pod_uuid, queue_name,
          project, department, nodepool) * on(pod_uuid, queue_name, project, department,
          nodepool) runai_allocated_gpu_count_per_pod) by (queue_name, project, department,
          nodepool) / on(queue_name, project, department, nodepool) (runai_allocated_gpu_count_per_queue>0)
        record: runai_gpu_utilization_per_queue
      - expr: sum(avg(runai_gpu_utilization_per_pod_per_gpu) by(pod_uuid, workload_type,
          project, department) * on(pod_uuid, workload_type, project, department)
          runai_allocated_gpu_count_per_pod) by (workload_type, project, department)
          / on(workload_type, project, department) (runai_allocated_gpu_count_per_project_per_workload_type>0)
        record: runai_gpu_utilization_per_project_per_workload_type
      - expr: sum(avg(runai_gpu_utilization_per_pod_per_gpu) by(pod_uuid, project,
          department) * on(pod_uuid, project, department) runai_allocated_gpu_count_per_pod)
          by (project, department) / on(project, department) (runai_allocated_gpu_count_per_project>0)
        record: runai_gpu_utilization_per_project
      - expr: sum(avg(runai_gpu_utilization_per_pod_per_gpu) by(pod_uuid, department)
          * on(pod_uuid, department) runai_allocated_gpu_count_per_pod) by (department)
          / on(department) (runai_allocated_gpu_count_per_department>0)
        record: runai_gpu_utilization_per_department
      - expr: sum by (UUID, gpu, node, nodepool, modelName) (runai_gpu_utilization_per_mig_pod
          * on(pod_uuid) group_left() runai_allocated_gpu_count_per_pod) or on(UUID)
          avg by (UUID, gpu, node, nodepool, modelName) (runai_dcgm_gpu_utilization)
          or on(UUID) avg by(UUID, gpu, node, nodepool, modelName) (runai_amd_gpu_utilization)
        record: runai_gpu_utilization_per_gpu
      - expr: timestamp(runai_gpu_utilization_per_gpu > 2) or on(UUID) runai_last_gpu_utilization_time_per_gpu
          or on(UUID) timestamp(runai_gpu_utilization_per_gpu)
        record: runai_last_gpu_utilization_time_per_gpu
      - expr: time() - runai_last_gpu_utilization_time_per_gpu
        record: runai_gpu_idle_seconds_per_gpu
      - expr: avg by(node, nodepool, modelName, ready, schedulable) (runai_gpu_utilization_per_gpu)
        record: runai_gpu_utilization_per_node
      - expr: avg by(nodepool) (runai_gpu_utilization_per_gpu)
        record: runai_gpu_utilization_per_nodepool
      - expr: avg(runai_gpu_utilization_per_gpu)
        record: runai_gpu_utilization_per_cluster
      - expr: "label_replace(label_replace(label_replace(label_replace(DCGM_FI_DEV_FB_USED,
          \"pod_ip\", \"$1\", \"instance\", \"(.*):(.*)\"), \"pod_name\", \"$1\",
          \"exported_pod\", \"(.+)\"), \"pod_namespace\", \"$1\", \"exported_namespace\",
          \"(.+)\"), \"gpu_instance_id\",\n\t\"$1\", \"GPU_I_ID\", \"(.+)\") * on
          (pod_ip) group_left(node) kube_pod_info{created_by_name=~\".*dcgm-exporter\",
          pod_ip!=\"\"} * on(node) group_left(nodepool) runai_node_nodepool_excluded"
        record: runai_dcgm_gpu_used_mebibytes
      - expr: label_join(label_replace(label_replace(label_replace(label_replace(amd_gpu_memory_used/(1024*1024),
          "node", "$1", "node_id", "(.*)"), "modelName", "$1", "productname", "(.*)"),
          "gpu", "$1", "gpu_id", "(.*)"), "pod_uuid", "$1", "pod", "(.*)"), "UUID",
          "-", "node", "gpu") * on(node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_amd_gpu_memory_used_mebibytes
      - expr: runai_pod_info_enriched_nodepool_unique{gpu!="", gpu_instance_id=""}
          * on (pod_uuid) group_right(node, nodepool, project, department, queue_name,
          pod_name, pod_namespace, workload_name, workload_type, workload_id, modelName)
          runai_pod_gpu_memory_used_bytes/(1024*1024) or on(pod_uuid) runai_pod_info_enriched_nodepool_unique{gpu!="",
          gpu_instance_id=""}*0
        record: runai_gpu_memory_used_mebibytes_per_fractional_pod
      - expr: runai_pod_info_enriched_nodepool_unique{gpu="", gpu_instance_id=""}
          * on(pod_name, pod_namespace) group_right(workload_name, workload_type,
          workload_id, pod_uuid, project, department, queue_name) runai_dcgm_gpu_used_mebibytes
          or on(pod_uuid) runai_pod_info_enriched_nodepool_unique{gpu="", gpu_instance_id=""}
          * on(pod_uuid) group_right(workload_name, workload_type, workload_id, pod_name,
          pod_namespace, project, department, queue_name) runai_amd_gpu_memory_used_mebibytes
        record: runai_gpu_memory_used_mebibytes_per_whole_pod_per_gpu
      - expr: runai_pod_info_enriched_nodepool_unique{gpu!="", gpu_instance_id!=""}
          * on(node, gpu, gpu_instance_id) group_right(pod_name, pod_namespace, workload_name,
          workload_type, workload_id, pod_uuid, project, department, queue_name) runai_dcgm_gpu_used_mebibytes
        record: runai_gpu_memory_used_mebibytes_per_mig_pod
      - expr: sum by(node, nodepool, project, department, queue_name, pod_name, pod_uuid,
          pod_namespace, workload_name, workload_type, workload_id, gpu, modelName,
          service) (runai_gpu_memory_used_mebibytes_per_fractional_pod or on (pod_uuid)
          runai_gpu_memory_used_mebibytes_per_mig_pod or on (pod_uuid) runai_gpu_memory_used_mebibytes_per_whole_pod_per_gpu)
        record: runai_gpu_memory_used_mebibytes_per_pod_per_gpu
      - expr: sum(runai_gpu_memory_used_mebibytes_per_pod_per_gpu) by (nodepool, project,
          department, queue_name, workload_name, workload_type, workload_id, user)
          * on(workload_id) group_left(nodepool, project, department, queue_name,
          workload_name, workload_type, user, is_gpu, phase, detailed_status, status)
          runai_workload_info_enriched
        record: runai_gpu_memory_used_mebibytes_per_workload
      - expr: sum(runai_gpu_memory_used_mebibytes_per_workload) by (workload_type,
          queue_name, project, department, nodepool)
        record: runai_gpu_memory_used_mebibytes_per_queue_per_workload_type
      - expr: sum by (UUID, gpu, node, nodepool, modelName) (runai_dcgm_gpu_used_mebibytes)
          or on(UUID) sum by(UUID, gpu, node, nodepool, modelName) (runai_amd_gpu_memory_used_mebibytes)
        record: runai_gpu_memory_used_mebibytes_per_gpu
      - expr: sum by(node, nodepool, modelName, ready, schedulable) (runai_gpu_memory_used_mebibytes_per_gpu)
        record: runai_gpu_memory_used_mebibytes_per_node
      - expr: sum by(nodepool) (runai_gpu_memory_used_mebibytes_per_node)
        record: runai_gpu_memory_used_mebibytes_per_nodepool
      - expr: sum(runai_gpu_memory_used_mebibytes_per_nodepool)
        record: runai_gpu_memory_used_mebibytes_per_cluster
      - expr: "label_replace(label_replace(label_replace(label_replace(DCGM_FI_DEV_FB_USED+DCGM_FI_DEV_FB_FREE,
          \"pod_ip\", \"$1\", \"instance\", \"(.*):(.*)\"), \"pod_name\", \"$1\",
          \"exported_pod\", \"(.+)\"), \"pod_namespace\", \"$1\", \"exported_namespace\",
          \"(.+)\"), \"gpu_instance_id\",\n\t\"$1\", \"GPU_I_ID\", \"(.+)\") * on
          (pod_ip) group_left(node) kube_pod_info{created_by_name=~\".*dcgm-exporter\",
          pod_ip!=\"\"} * on(node) group_left(nodepool) runai_node_nodepool_excluded"
        record: runai_dcgm_gpu_total_mebibytes
      - expr: label_join(label_replace(label_replace(label_replace(label_replace(amd_gpu_memory_total/(1024*1024),
          "node", "$1", "node_id", "(.*)"), "modelName", "$1", "productname", "(.*)"),
          "gpu", "$1", "gpu_id", "(.*)"), "pod_uuid", "$1", "pod", "(.*)"), "UUID",
          "-", "node", "gpu") * on(node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_amd_gpu_memory_total_mebibytes
      - expr: (group by (UUID, gpu, node, nodepool, modelName) (runai_dcgm_gpu_total_mebibytes{gpu_instance_id!="",
          modelName=~".*H100.*"}))*80000 or (group by (UUID, gpu, node, nodepool,
          modelName) (runai_dcgm_gpu_total_mebibytes{gpu_instance_id!="", modelName=~".*40GB"}))*40000
          or (group by (UUID, gpu, node, nodepool, modelName) (runai_dcgm_gpu_total_mebibytes{gpu_instance_id!="",
          modelName=~".*80GB"}))*80000
        record: runai_gpu_memory_total_mebibytes_per_mig_mode_gpu
      - expr: runai_gpu_memory_total_mebibytes_per_mig_mode_gpu or on(UUID) sum by
          (UUID, gpu, node, nodepool, modelName) (runai_dcgm_gpu_total_mebibytes)
          or on(UUID) sum by(UUID, gpu, node, nodepool, modelName) (runai_amd_gpu_memory_total_mebibytes)
        record: runai_gpu_memory_total_mebibytes_per_gpu
      - expr: sum by(node, nodepool, modelName, ready, schedulable) (runai_gpu_memory_total_mebibytes_per_gpu)
        record: runai_gpu_memory_total_mebibytes_per_node
      - expr: sum by(nodepool) (runai_gpu_memory_total_mebibytes_per_node)
        record: runai_gpu_memory_total_mebibytes_per_nodepool
      - expr: sum(runai_gpu_memory_total_mebibytes_per_nodepool)
        record: runai_gpu_memory_total_mebibytes_per_cluster
      - expr: 100*(runai_gpu_memory_used_mebibytes_per_node / runai_gpu_memory_total_mebibytes_per_node)
        record: runai_gpu_memory_utilization_per_node
      - expr: 100*(runai_gpu_memory_used_mebibytes_per_nodepool / runai_gpu_memory_total_mebibytes_per_nodepool)
        record: runai_gpu_memory_utilization_per_nodepool
      - expr: 100*(runai_gpu_memory_used_mebibytes_per_cluster / runai_gpu_memory_total_mebibytes_per_cluster)
        record: runai_gpu_memory_utilization_per_cluster
      - expr: sum by(nodepool, project, department, queue_name, workload_name, workload_type,
          workload_id, user, is_gpu, phase, detailed_status, status) (sum(label_replace(sum(kube_pod_container_resource_requests{resource='cpu'})
          by(uid), "pod_uuid" , "$1", "uid", "(.*)") * on(pod_uuid) group_right()
          runai_pod_info_enriched_nodepool_unique or on(pod_uuid) runai_pod_info_enriched_nodepool_unique*0)
          by(workload_id) * on(workload_id) group_right() runai_workload_info_enriched{detailed_status="Pending"})
        record: runai_pending_requested_cpu_cores_per_workload
      - expr: sum(runai_pending_requested_cpu_cores_per_workload) by (queue_name,
          project, department, nodepool)
        record: runai_pending_requested_cpu_cores_per_queue
      - expr: sum(runai_pending_requested_cpu_cores_per_queue) by (project, department)
        record: runai_pending_requested_cpu_cores_per_project
      - expr: sum(runai_pending_requested_cpu_cores_per_queue) by (department)
        record: runai_pending_requested_cpu_cores_per_department
      - expr: sum(runai_pending_requested_cpu_cores_per_queue) by (nodepool)
        record: runai_pending_requested_cpu_cores_per_nodepool
      - expr: sum(runai_pending_requested_cpu_cores_per_nodepool)
        record: runai_pending_requested_cpu_cores_per_cluster
      - expr: sum by(node, nodepool, project, department, queue_name, pod_name, pod_uuid,
          pod_namespace, workload_name, workload_type, workload_id, gpu, modelName,
          service) (runai_scheduler_allocated_millicpus_per_pod_unique) * on (pod_uuid)
          group_right() runai_pod_info_enriched_nodepool_unique
        record: runai_allocated_millicpus_per_pod
      - expr: sum by(node, nodepool, project, department, queue_name, pod_name, pod_uuid,
          pod_namespace, workload_name, workload_type, workload_id, gpu, modelName,
          service) ((runai_scheduler_allocated_millicpus_per_pod_unique/1000) * on
          (pod_uuid) group_right() runai_pod_info_enriched_nodepool_unique)
        record: runai_allocated_cpu_cores_per_pod
      - expr: sum(runai_allocated_cpu_cores_per_pod) by (nodepool, project, department,
          queue_name, workload_name, workload_type, workload_id, user) * on(workload_id)
          group_left(nodepool, project, department, queue_name, workload_name, workload_type,
          user, is_gpu, phase, detailed_status, status) runai_workload_info_enriched
        record: runai_allocated_cpu_cores_per_workload
      - expr: sum(runai_allocated_cpu_cores_per_workload) by (queue_name, project,
          department, nodepool) or on(queue_name) group(runai_queue_info) by(queue_name,
          project, department, nodepool)*0
        record: runai_allocated_cpu_cores_per_queue
      - expr: sum(runai_allocated_cpu_cores_per_queue) by (project, department)
        record: runai_allocated_cpu_cores_per_project
      - expr: sum(runai_allocated_cpu_cores_per_queue) by (department)
        record: runai_allocated_cpu_cores_per_department
      - expr: sum by(node, nodepool, modelName, ready, schedulable) (sum by (node)
          (kube_pod_container_resource_requests{resource="cpu",unit="core",node!=""}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1))
          * on(node) group_right() runai_node_info_enriched)
        record: runai_allocated_cpu_cores_per_node
      - expr: sum(runai_allocated_cpu_cores_per_node) by (nodepool)
        record: runai_allocated_cpu_cores_per_nodepool
      - expr: sum(runai_allocated_cpu_cores_per_nodepool)
        record: runai_allocated_cpu_cores_per_cluster
      - expr: sum by(node, nodepool, project, department, queue_name, pod_name, pod_uuid,
          pod_namespace, workload_name, workload_type, workload_id, gpu, modelName,
          service) (label_replace(label_replace(sum(rate(container_cpu_usage_seconds_total{container!=""}[2m]))
          by (pod, namespace),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" ,
          "$1", "namespace", "(.*)") * on(pod_name, pod_namespace) group_right() runai_pod_info_enriched_nodepool_unique)
        record: runai_used_cpu_cores_per_pod
      - expr: sum(runai_used_cpu_cores_per_pod) by (nodepool, project, department,
          queue_name, workload_name, workload_type, workload_id, user) * on(workload_id)
          group_left(nodepool, project, department, queue_name, workload_name, workload_type,
          user, is_gpu, phase, detailed_status, status) runai_workload_info_enriched
        record: runai_used_cpu_cores_per_workload
      - expr: sum(runai_used_cpu_cores_per_workload) by (queue_name, project, department,
          nodepool)
        record: runai_used_cpu_cores_per_queue
      - expr: sum(runai_used_cpu_cores_per_queue) by (project, department)
        record: runai_used_cpu_cores_per_project
      - expr: sum(runai_used_cpu_cores_per_queue) by (department)
        record: runai_used_cpu_cores_per_department
      - expr: sum by(node, nodepool, modelName, ready, schedulable) ((runai_cpu_cores_count_per_node
          - on(node) sum(rate(node_cpu_seconds_total{job="node-exporter",mode=~"idle|iowait|steal"}[1m]))
          by(pod, namespace) * on(pod, namespace) group_left(node) kube_pod_info)
          * on(node) group_right() runai_node_info_enriched)
        record: runai_used_cpu_cores_per_node
      - expr: sum(runai_used_cpu_cores_per_node) by (nodepool)
        record: runai_used_cpu_cores_per_nodepool
      - expr: sum(runai_used_cpu_cores_per_nodepool)
        record: runai_used_cpu_cores_per_cluster
      - expr: sum(runai_queue_quota_cpu_cores) by (queue_name, project, department,
          nodepool)
        record: runai_deserved_cpu_cores_per_queue
      - expr: min(runai_deserved_cpu_cores_per_queue{project!=""}<0) by(project, department)
          or on(project) sum(runai_deserved_cpu_cores_per_queue{project!=""}>=0) by(project,
          department)
        record: runai_deserved_cpu_cores_per_project
      - expr: min(runai_deserved_cpu_cores_per_queue{project=""}<0) by(department)
          or on(department) sum(runai_deserved_cpu_cores_per_queue{project=""}>=0)
          by(department)
        record: runai_deserved_cpu_cores_per_department
      - expr: sum by (node, nodepool, modelName, ready, schedulable) (kube_node_status_capacity{resource='cpu'}
          * on(node) group_right() runai_node_info_enriched)
        record: runai_cpu_cores_count_per_node
      - expr: sum(runai_cpu_cores_count_per_node) by (nodepool)
        record: runai_cpu_cores_count_per_nodepool
      - expr: sum(runai_cpu_cores_count_per_node{ready="true",schedulable="true"})
          by (nodepool)
        record: runai_ready_cpu_cores_count_per_nodepool
      - expr: sum(runai_cpu_cores_count_per_nodepool)
        record: runai_cpu_cores_count_per_cluster
      - expr: sum(runai_ready_cpu_cores_count_per_nodepool)
        record: runai_ready_cpu_cores_count_per_cluster
      - expr: 100*(runai_allocated_cpu_cores_per_node / runai_cpu_cores_count_per_node>0)
        record: runai_cpu_cores_allocation_percentage_per_node
      - expr: 100*(runai_allocated_cpu_cores_per_nodepool / runai_cpu_cores_count_per_nodepool>0)
        record: runai_cpu_cores_allocation_percentage_per_nodepool
      - expr: 100*(runai_allocated_cpu_cores_per_cluster / runai_cpu_cores_count_per_cluster>0)
        record: runai_cpu_cores_allocation_percentage_per_cluster
      - expr: 100*(runai_used_cpu_cores_per_pod / (runai_allocated_cpu_cores_per_pod>0))
          or on(pod_uuid) (runai_used_cpu_cores_per_pod*0)+100
        record: runai_cpu_utilization_per_pod
      - expr: 100*(runai_used_cpu_cores_per_workload / (runai_allocated_cpu_cores_per_workload>0))
          or on(workload_id) (runai_used_cpu_cores_per_workload*0)+100
        record: runai_cpu_utilization_per_workload
      - expr: 100*(runai_used_cpu_cores_per_queue / (runai_allocated_cpu_cores_per_queue>0))
          or on(queue_name) (runai_used_cpu_cores_per_queue*0)+100
        record: runai_cpu_utilization_per_queue
      - expr: 100*(runai_used_cpu_cores_per_project / (runai_allocated_cpu_cores_per_project>0))
          or on(project) (runai_used_cpu_cores_per_project*0)+100
        record: runai_cpu_utilization_per_project
      - expr: 100*(runai_used_cpu_cores_per_department / (runai_allocated_cpu_cores_per_department>0))
          or on(department) (runai_used_cpu_cores_per_department*0)+100
        record: runai_cpu_utilization_per_department
      - expr: 100*(runai_used_cpu_cores_per_node / runai_cpu_cores_count_per_node)
        record: runai_cpu_utilization_per_node
      - expr: 100*(runai_used_cpu_cores_per_nodepool / runai_cpu_cores_count_per_nodepool)
        record: runai_cpu_utilization_per_nodepool
      - expr: 100*(runai_used_cpu_cores_per_cluster / runai_cpu_cores_count_per_cluster)
        record: runai_cpu_utilization_per_cluster
      - expr: sum by(nodepool, project, department, queue_name, workload_name, workload_type,
          workload_id, user, is_gpu, phase, detailed_status, status) (sum(label_replace(sum(kube_pod_container_resource_requests{resource='memory'})
          by(uid), "pod_uuid" , "$1", "uid", "(.*)") * on(pod_uuid) group_right()
          runai_pod_info_enriched_nodepool_unique or on(pod_uuid) runai_pod_info_enriched_nodepool_unique*0)
          by(workload_id) * on(workload_id) group_right() runai_workload_info_enriched{detailed_status="Pending"})
        record: runai_pending_requested_memory_bytes_per_workload
      - expr: sum(runai_pending_requested_memory_bytes_per_workload) by (queue_name,
          project, department, nodepool)
        record: runai_pending_requested_memory_bytes_per_queue
      - expr: sum(runai_pending_requested_memory_bytes_per_queue) by (project, department)
        record: runai_pending_requested_memory_bytes_per_project
      - expr: sum(runai_pending_requested_memory_bytes_per_queue) by (department)
        record: runai_pending_requested_memory_bytes_per_department
      - expr: sum(runai_pending_requested_memory_bytes_per_queue) by (nodepool)
        record: runai_pending_requested_memory_bytes_per_nodepool
      - expr: sum(runai_pending_requested_memory_bytes_per_nodepool)
        record: runai_pending_requested_memory_bytes_per_cluster
      - expr: sum by(node, nodepool, project, department, queue_name, pod_name, pod_uuid,
          pod_namespace, workload_name, workload_type, workload_id, gpu, modelName,
          service) (runai_scheduler_allocated_memory_bytes_per_pod_unique) * on (pod_uuid)
          group_right() runai_pod_info_enriched_nodepool_unique
        record: runai_allocated_memory_per_pod
      - expr: sum by(node, nodepool, modelName, ready, schedulable) (sum by (node)
          (kube_pod_container_resource_requests{resource="memory",unit="byte",node!=""}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1))
          * on(node) group_right() runai_node_info_enriched)
        record: runai_allocated_memory_bytes_per_node
      - expr: sum(runai_allocated_memory_bytes_per_node) by (nodepool)
        record: runai_allocated_memory_bytes_per_nodepool
      - expr: sum(runai_allocated_memory_bytes_per_nodepool)
        record: runai_allocated_memory_bytes_per_cluster
      - expr: sum by(node, nodepool, project, department, queue_name, pod_name, pod_uuid,
          pod_namespace, workload_name, workload_type, workload_id, gpu, modelName,
          service) (label_replace(label_replace(sum(container_memory_usage_bytes{
          container!=""}) by (pod, namespace),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace"
          , "$1", "namespace", "(.*)") * on(pod_name, pod_namespace) group_right()
          runai_pod_info_enriched_nodepool_unique)
        record: runai_used_memory_bytes_per_pod
      - expr: sum by(node, nodepool, modelName, ready, schedulable) ((node_memory_MemTotal_bytes{job="node-exporter"}
          - (node_memory_MemAvailable_bytes{job="node-exporter"} or (node_memory_Cached_bytes{job="node-exporter"}
          + node_memory_Buffers_bytes{job="node-exporter"} + node_memory_MemFree_bytes{job="node-exporter"}
          + node_memory_Slab_bytes{job="node-exporter"}))) * on(pod, namespace) group_left(node)
          kube_pod_info * on(node) group_right() runai_node_info_enriched)
        record: runai_used_memory_bytes_per_node
      - expr: sum(runai_used_memory_bytes_per_node) by (nodepool)
        record: runai_used_memory_bytes_per_nodepool
      - expr: sum(runai_used_memory_bytes_per_nodepool)
        record: runai_used_memory_bytes_per_cluster
      - expr: sum by(queue_name, project, department, nodepool) (runai_queue_quota_memory_bytes)
        record: runai_deserved_memory_bytes_per_queue
      - expr: min(runai_deserved_memory_bytes_per_queue{project!=""}<0) by(project,
          department) or on(project) sum(runai_deserved_memory_bytes_per_queue{project!=""}>=0)
          by(project, department)
        record: runai_deserved_memory_bytes_per_project
      - expr: min(runai_deserved_memory_bytes_per_queue{project=""}<0) by(department)
          or on(department) sum(runai_deserved_memory_bytes_per_queue{project=""}>=0)
          by(department)
        record: runai_deserved_memory_bytes_per_department
      - expr: sum by (node, nodepool, modelName, ready, schedulable) (kube_node_status_capacity{resource='memory'}
          * on(node) group_right() runai_node_info_enriched)
        record: runai_total_memory_bytes_per_node
      - expr: sum(runai_total_memory_bytes_per_node) by (nodepool)
        record: runai_total_memory_bytes_per_nodepool
      - expr: sum(runai_total_memory_bytes_per_nodepool)
        record: runai_total_memory_bytes_per_cluster
      - expr: 100*(runai_allocated_memory_bytes_per_node / runai_total_memory_bytes_per_node>0)
        record: runai_memory_allocation_percentage_per_node
      - expr: 100*(runai_allocated_memory_bytes_per_nodepool / runai_total_memory_bytes_per_nodepool>0)
        record: runai_memory_allocation_percentage_per_nodepool
      - expr: 100*(runai_allocated_memory_bytes_per_cluster / runai_total_memory_bytes_per_cluster>0)
        record: runai_memory_allocation_percentage_per_cluster
      - expr: 100*(runai_used_memory_bytes_per_node / runai_total_memory_bytes_per_node)
        record: runai_memory_utilization_per_node
      - expr: 100*(runai_used_memory_bytes_per_nodepool / runai_total_memory_bytes_per_nodepool)
        record: runai_memory_utilization_per_nodepool
      - expr: 100*(runai_used_memory_bytes_per_cluster / runai_total_memory_bytes_per_cluster)
        record: runai_memory_utilization_per_cluster
      - expr: count(runai_workload_info_enriched) by(detailed_status, is_gpu, workload_type,
          queue_name, project, department, nodepool)
        record: runai_workload_count_per_type_per_phase_per_queue
      - expr: sum(runai_workload_count_per_type_per_phase_per_queue) by(detailed_status,
          is_gpu, workload_type, nodepool)
        record: runai_workload_count_per_type_per_phase_per_nodepool
      - expr: sum(runai_workload_count_per_type_per_phase_per_nodepool) by(detailed_status,
          is_gpu, workload_type)
        record: runai_workload_count_per_type_per_phase_per_cluster
      - expr: sum(runai_pod_mig_used_memory) by (node_name, node)
        record: runai_mig_gpu_used_memory
      - expr: count(runai_allocated_gpu_count_per_workload{workload_type=~"Interactive|interactive|Interactive-Preemptible"}>0)
          by(nodepool)
        record: runai_workloads_count_gpu_interactive
      - expr: count(runai_allocated_gpu_count_per_workload{workload_type=~"Train|train"}>0)
          by(nodepool)
        record: runai_workloads_count_gpu_train
      - expr: count(runai_allocated_gpu_count_per_workload{workload_type=~"Inference|inference"}>0)
          by(nodepool)
        record: runai_workloads_count_gpu_inference
      - expr: count((runai_allocated_gpu_count_per_workload==0) * on(workload_id,
          nodepool) (group(runai_workload_info_enriched{detailed_status=~"^$|Running"}==1)
          by(workload_id, nodepool))) by(nodepool)
        record: runai_workloads_count_cpu_only
      - expr: (node_cpu_seconds_total * on(pod, namespace) group_left(node) kube_pod_info)
          * on(node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_node_cpu_seconds_total
      - expr: (node_memory_MemAvailable_bytes * on(pod, namespace) group_left(node)
          kube_pod_info) * on(node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_node_memory_mem_available_bytes
      - expr: (node_memory_MemTotal_bytes * on(pod, namespace) group_left(node) kube_pod_info)
          * on(node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_node_memory_mem_total_bytes
    - name: runai-dashboard-rules
      rules:
      - expr: sum(runai_allocated_gpu_count_per_queue>0) by(queue_name, project)
        record: runai_dashboard:overview:allocated_gpus_per_project:queue_name
      - expr: (runai_gpu_idle_seconds_per_workload > 300)*0 + on(workload_id) group_left()
          (runai_allocated_gpu_count_per_workload > 0)
        record: runai_idle_allocated_gpu_count_per_workload
      - expr: (runai_gpu_idle_seconds_per_pod > 300)*0 + on(node, nodepool, project,
          department, queue_name, pod_name, pod_namespace, workload_name, workload_type,
          workload_id, modelName) group_left() (runai_allocated_gpu_count_per_pod
          > 0)
        record: runai_idle_allocated_gpu_count_per_pod
      - expr: sum(runai_idle_allocated_gpu_count_per_workload) by (queue_name, project,
          department, nodepool)
        record: runai_idle_allocated_gpu_count_per_queue
      - expr: sum(runai_idle_allocated_gpu_count_per_pod) by (node, nodepool, modelName,
          ready, schedulable)
        record: runai_idle_allocated_gpu_count_per_node
      - expr: sum(runai_idle_allocated_gpu_count_per_workload) by (nodepool)
        record: runai_idle_allocated_gpu_count_per_nodepool
      - expr: runai_gpu_idle_seconds_per_workload > 300
        record: runai_dashboard:overview:workloads_with_idle_allocated_gpus:idle_time
      - expr: 100*(sum(runai_scheduler_allocated_gpus_per_pod_unique)/clamp_min(sum(runai_gpu_count_per_node),
          1)) or vector(0)
        record: runai_dashboard:analytics:gpu_allocation
      - expr: 100*(sum(runai_scheduler_allocated_gpus_per_pod_unique)/clamp_min(sum(runai_gpu_count_per_node),
          1)) or vector(0)
        record: runai_dashboard:analytics:gpu_memory_allocation
      - expr: avg(runai_gpu_utilization_per_gpu) or vector(0)
        record: runai_dashboard:analytics:gpu_compute_utilization
      - expr: 100*(sum(runai_gpu_memory_used_mebibytes_per_gpu)/sum(runai_gpu_memory_total_mebibytes_per_gpu))
          or vector(0)
        record: runai_dashboard:analytics:gpu_memory_utilization
      - expr: sum(runai_gpu_count_per_node) or vector(0)
        record: runai_dashboard:analytics:allocated_gpus_per_project:total
      - expr: sum(runai_allocated_gpu_count_per_workload) or vector(0)
        record: runai_dashboard:analytics:allocated_gpus_per_project:total_allocated
      - expr: sum(runai_allocated_gpu_count_per_project>0) by(project)
        record: runai_dashboard:analytics:allocated_gpus_per_project:project
      - expr: sum(runai_allocated_gpu_count_per_queue>0) by (queue_name, project)
        record: runai_dashboard:analytics:allocated_gpus_per_project:queue_name
      - expr: runai_gpu_utilization_per_project
        record: runai_dashboard:analytics:gpu_compute_utilization_per_project:project
      - expr: sum(runai_gpu_utilization_per_queue) by (queue_name, project)
        record: runai_dashboard:analytics:gpu_compute_utilization_per_project:queue_name
      - expr: runai_gpu_utilization_per_node
        record: runai_dashboard:analytics:gpu_compute_utilization_per_node:node
      - expr: 100*(runai_allocated_gpu_count_per_node / on(node) (runai_gpu_count_per_node>0))
        record: runai_dashboard:analytics:gpu_allocation_per_node:node
      - expr: runai_gpu_count_per_node>0
        record: runai_dashboard:analytics:gpu_count_per_node:node
      - expr: sum(runai_gpu_count_per_node) or vector(0)
        record: runai_dashboard:analytics:gpu_count_per_node:total
      - expr: runai_gpu_memory_total_mebibytes_per_node
        record: runai_dashboard:analytics:gpu_memory_size_per_node:node
      - expr: sum(runai_gpu_memory_total_mebibytes_per_node) or vector(0)
        record: runai_dashboard:analytics:gpu_memory_size_per_node:total
      - expr: 100*(runai_allocated_gpu_count_per_node / on(node, nodepool) (runai_gpu_count_per_node>0))
        record: runai_dashboard:analytics:gpu_memory_allocation_per_node:node
      - expr: 100*(runai_gpu_memory_used_mebibytes_per_node / on(node, nodepool) runai_gpu_memory_total_mebibytes_per_node)
        record: runai_dashboard:analytics:gpu_memory_utilization_per_node:node
    - name: runai-rules
      rules:
      - expr: runai_pod_info_enriched_nodepool_unique
        record: runai_pod_phase_with_info
      - expr: sum(runai_scheduler_requested_gpus_per_pod_unique) by (nodepool, project,
          department, queue_name, workload_name, workload_type, workload_id, user)
          * on(workload_id) group_left(nodepool, project, department, queue_name,
          workload_name, workload_type, user, is_gpu, phase, detailed_status, status)
          runai_workload_info_enriched
        record: runai_requested_gpus_per_workload
      - expr: sum(runai_scheduler_requested_gpu_memory_mb_per_pod_unique) by (nodepool,
          project, department, queue_name, workload_name, workload_type, workload_id,
          user) * on(workload_id) group_left(nodepool, project, department, queue_name,
          workload_name, workload_type, user, is_gpu, phase, detailed_status, status)
          runai_workload_info_enriched
        record: runai_requested_gpu_memory_mb_per_workload
      - expr: sum(runai_scheduler_gpus_limit_per_pod_unique) by (nodepool, project,
          department, queue_name, workload_name, workload_type, workload_id, user)
          * on(workload_id) group_left(nodepool, project, department, queue_name,
          workload_name, workload_type, user, is_gpu, phase, detailed_status, status)
          runai_workload_info_enriched
        record: runai_gpus_limit_per_workload
      - expr: sum(runai_scheduler_gpu_memory_mb_limit_per_pod_unique) by (nodepool,
          project, department, queue_name, workload_name, workload_type, workload_id,
          user) * on(workload_id) group_left(nodepool, project, department, queue_name,
          workload_name, workload_type, user, is_gpu, phase, detailed_status, status)
          runai_workload_info_enriched
        record: runai_gpu_memory_mb_limit_per_workload
      - expr: sum(max(runai_requested_gpus_per_workload) by (workload_id) * on (workload_id)
          group_left (queue_name, nodepool, workload_type) (runai_workload_info_enriched{phase="Pending",workload_type!~"Inference|inference|"}==1))
          by (nodepool, workload_type, queue_name)
        record: runai_requested_gpus_per_workload_type_per_queue
      - expr: 1 -  avg without(cpu, mode) (rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[5m]))
        record: runai_node_cpu_utilisation:rate5m
      - expr: label_replace(label_replace(label_replace(kube_pod_info, "pod_uuid",
          "$1", "uid", "(.*)"),"pod_name", "$1", "pod", "(.*)"), "pod_namespace",
          "$1", "namespace", "(.*)")
        record: runai_kube_pod_info
      - expr: sum by ( gpu, node, pod_name, pod_namespace) ((label_replace(runai_dcgm_gpu_utilization,
          "pod_ip", "$1", "instance", "(.*):(.*)")) * on (pod_ip) group_left(node)
          kube_pod_info{created_by_name=~".*dcgm-exporter", pod_ip!=""})
        record: runai_node_gpu_utilization
      - expr: (sum without (container, job, namespace, pod, service, endpoint, instance)
          (runai_node_cpu_utilisation:rate5m * on (pod, namespace) group_left(node)
          (kube_pod_info{pod_ip!=""} *0+1))) * on (node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_node_cpu_utilization
      - expr: avg(1-  (rate(runai_node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m])))
          by (nodepool)
        record: runai_cpu_utilization
      - expr: sum by (workload_name, workload_id, pod_namespace, project) (label_replace(label_replace(sum(rate(container_cpu_usage_seconds_total{
          container!=""}[2m])) by (pod, namespace),"pod_name" , "$1", "pod", "(.*)"),
          "pod_namespace" , "$1", "namespace", "(.*)") * on(pod_name, pod_namespace)
          group_left(workload_name, workload_id, project) (runai_pod_phase_with_info{phase="Running"}
          ==1))
        record: runai_job_cpu_usage
      - expr: sum by (pod, namespace) (rate(container_cpu_usage_seconds_total{container=""}[2m]))
          * on (pod, namespace) group_right() label_replace(label_replace(runai_pod_phase_with_info{phase="Running"}
          == 1, "pod" , "$1", "pod_name", "(.*)"), "namespace", "$1", "pod_namespace",
          "(.*)")
        record: runai_pod_cpu_usage
      - expr: (sum without (container, device, endpoint, instance, job, namespace,
          pod, pod_ip, service) (instance:node_memory_utilisation:ratio * on (pod,
          namespace) group_left(node) (kube_pod_info{pod_ip!=""} *0+1))) * on(node)
          group_left(nodepool) runai_node_nodepool_excluded
        record: runai_node_memory_utilization
      - expr: 1 - (sum (runai_node_memory_mem_available_bytes{job="node-exporter"})
          by (nodepool))  / (sum(runai_node_memory_mem_total_bytes{job="node-exporter"})
          by (nodepool))
        record: runai_memory_utilization
      - expr: sum without (container, endpoint,instance, job,namespace,pod,service)
          ((node_memory_MemTotal_bytes{job="node-exporter"} - node_memory_MemAvailable_bytes{job="node-exporter"}
          ) * on (pod, namespace) group_left(node) kube_pod_info{pod_ip!=""})
        record: runai_node_memory_used_bytes
      - expr: sum(runai_node_memory_used_bytes)
        record: runai_memory_used_bytes
      - expr: sum(label_replace(label_replace(sum(container_memory_usage_bytes{ container!=""})
          by (pod, namespace),"pod_name" , "$1", "pod", "(.*)"), "pod_namespace" ,
          "$1", "namespace", "(.*)") * on(pod_namespace, pod_name) group_left(workload_name,
          workload_id, project) (runai_pod_info_enriched_nodepool_unique * 0 + 1))
          by (workload_name, workload_id, project)
        record: runai_job_memory_used_bytes
      - expr: sum by (pod, namespace) (container_memory_usage_bytes{container=""})
          * on (pod, namespace)  group_right() label_replace(label_replace(runai_pod_phase_with_info{phase="Running"}
          == 1, "pod" , "$1", "pod_name", "(.*)"), "namespace", "$1", "pod_namespace",
          "(.*)")
        record: runai_pod_memory_used_bytes
      - expr: (sum(kube_pod_container_resource_requests{resource="memory",unit="byte",node!=""}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1))
          by (node)) * on(node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_node_requested_memory_bytes
      - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests{resource="memory",unit="byte"}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Running"}==1),
          "pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace",
          "(.*)")) by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name,
          workload_id) (runai_pod_info_enriched_nodepool_unique * 0 + 1) ) by (workload_name,
          workload_id)
        record: runai_active_job_memory_allocated_bytes
      - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests{resource="memory",unit="byte"}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),
          "pod_name" , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace",
          "(.*)")) by (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name,
          workload_id) (runai_pod_info_enriched_nodepool_unique * 0 + 1) ) by (workload_name,
          workload_id)
        record: runai_active_job_memory_requested_bytes
      - expr: (sum(kube_pod_container_resource_requests{resource="cpu",unit="core",node!=""}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1))
          by (node)) * on(node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_node_cpu_requested
      - expr: kube_node_status_capacity * on(node) group_left(nodepool) runai_node_nodepool_excluded
        record: runai_node_cpu_capacity
      - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests{resource="cpu",unit="core"}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Running"}==1),"pod_name"
          , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by
          (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name,
          workload_id) (runai_pod_info_enriched_nodepool_unique * 0 + 1) ) by (workload_name,
          workload_id)
        record: runai_active_job_cpu_allocated_cores
      - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_requests{resource="cpu",unit="core"}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),"pod_name"
          , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by
          (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name,
          workload_id) (runai_pod_info_enriched_nodepool_unique * 0 + 1) ) by (workload_name,
          workload_id)
        record: runai_active_job_cpu_requested_cores
      - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_limits{resource="cpu"}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),"pod_name"
          , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by
          (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name,
          workload_id) (runai_pod_info_enriched_nodepool_unique * 0 + 1) ) by (workload_name,
          workload_id)
        record: runai_cpu_limits_per_active_workload
      - expr: sum(sum(label_replace(label_replace(kube_pod_container_resource_limits{resource="memory"}
          * on(pod, namespace) group_left() (kube_pod_status_phase{phase=~"Pending|Running|Unknown"}==1),"pod_name"
          , "$1", "pod", "(.*)"), "pod_namespace" , "$1", "namespace", "(.*)")) by
          (pod_name, pod_namespace) * on(pod_name, pod_namespace) group_left(workload_name,
          workload_id) (runai_pod_info_enriched_nodepool_unique * 0 + 1) ) by (workload_name,
          workload_id)
        record: runai_memory_limits_per_active_workload
      - expr: avg(runai_cpu_utilization)
        record: runai_cluster_cpu_utilization
      - expr: (runai_memory_used_bytes)
        record: runai_cluster_memory_used_bytes
      - expr: (runai_memory_utilization)
        record: runai_cluster_memory_utilization
      - expr: (runai_node_cpu_requested)
        record: runai_node_cpu_requested_cores
      - expr: sum without(container, endpoint, instance, job, namespace, pod, service,
          exporter_container) (runai_node_memory_used_bytes)
        record: runai_node_used_memory_bytes
      - expr: sum without(endpoint, instance, job,namespace, pod, queue_name, service)(runai_queue_deserved_gpus{project!=""})
        record: runai_project_nodepool_guaranteed_gpus
      - expr: sum(runai_project_nodepool_guaranteed_gpus) by (project)
        record: runai_project_guaranteed_gpus
      - expr: sum without(endpoint, instance, job,namespace, pod, queue_name, service,
          deserved_cpu, deserved_gpu, deserved_memory, nodepool)(runai_queue_info{project!=""})
        record: runai_project_info
      - expr: sum without (revision_name) (label_replace((sum by(clusterId, revision_name,
          namespace_name) (rate(revision_app_request_count[1m]))),"deployment_name",
          "$1-deployment", "revision_name", "(.+)"))
        record: runai_deployment_request_rate
      - expr: sum without (revision_name) (label_replace( (sum by (clusterId, namespace_name,
          revision_name, le) (rate (revision_app_request_latencies_bucket[60s])))
          , "deployment_name", "$1-deployment", "revision_name", "(.+)"))
        record: runai_deployment_request_latencies
      - expr: (label_replace(rate(revision_app_request_count[1m]), "pod_namespace"
          , "$1", "namespace_name", "(.*)")) * on(pod_namespace, pod_name) group_left(department,
          namespace, node, nodepool, pod_uuid, workload_id, workload_name, workload_type)
          runai_pod_info_enriched_nodepool_unique
        record: runai_inference_throughput_per_pod
      - expr: (label_replace(rate(revision_app_request_latencies_sum[1m]) / (rate(revision_app_request_latencies_count[1m])
          > 0), "pod_namespace" , "$1", "namespace_name", "(.*)")) * on(pod_namespace,
          pod_name) group_left(department, namespace, node, nodepool, pod_uuid, workload_id,
          workload_name, workload_type) runai_pod_info_enriched_nodepool_unique
        record: runai_inference_latency_per_pod
    - interval: 30s
      name: runai-rules-short-interval
      rules:
      - expr: sum_over_time(avg(runai_workload_info_unique{detailed_status="Running"})
          by (workload_name, workload_id, workload_type)[30s:1s])
        record: runai_workload_run_time_per_30second
      - expr: ((runai_run_time_seconds_per_workload + runai_workload_run_time_per_30second)
          or runai_run_time_seconds_per_workload or (runai_workload_run_time_per_30second
          * 0)) and last_over_time(runai_workload_run_time_per_30second[120s])
        record: runai_run_time_seconds_per_workload
      - expr: sum_over_time(avg(runai_workload_info_unique{detailed_status="Pending"})
          by (workload_name, workload_id, workload_type)[30s:1s])
        record: runai_workload_wait_time_per_30second
      - expr: ((runai_wait_time_seconds_per_workload + runai_workload_wait_time_per_30second)
          or runai_wait_time_seconds_per_workload or (runai_workload_wait_time_per_30second
          * 0)) and last_over_time(runai_workload_wait_time_per_30second[120s])
        record: runai_wait_time_seconds_per_workload
    - interval: 1h
      name: runai-rules-hourly-interval
      rules:
      - expr: sum_over_time(runai_allocated_gpu_count_per_pod[1h:1s])
        record: runai_allocated_gpu_count_per_pod:hourly
      - expr: sum_over_time(runai_allocated_millicpus_per_pod[1h:1s])
        record: runai_allocated_millicpus_per_pod:hourly
      - expr: sum_over_time(runai_allocated_memory_per_pod[1h:1s])
        record: runai_allocated_memory_per_pod:hourly
      - expr: sum_over_time(runai_used_cpu_cores_per_pod[1h:1s])
        record: runai_used_cpu_cores_per_pod:hourly
      - expr: sum_over_time(runai_used_memory_bytes_per_pod[1h:1s])
        record: runai_used_memory_bytes_per_pod:hourly
      - expr: sum_over_time(runai_idle_allocated_gpu_count_per_queue[1h:1s])
        record: runai_gpu_idle_hours_per_queue:hourly
      - expr: sum_over_time(runai_allocated_gpu_count_per_project[1h:1s])
        record: runai_allocated_gpu_count_per_project:hourly
      - expr: sum_over_time(runai_allocated_gpu_count_per_department[1h:1s])
        record: runai_allocated_gpu_count_per_department:hourly
      - expr: sum_over_time(runai_gpu_utilization_per_project[1h:1s])
        record: runai_gpu_utilization_per_project:hourly
      - expr: sum_over_time(runai_gpu_utilization_per_department[1h:1s])
        record: runai_gpu_utilization_per_department:hourly
    - name: runai-alerts
      rules:
      - alert: RunaiAgentPullRateLow
        annotations:
          summary: Runai Agent fails pulling configuration from the backend (instance
            {{ $labels.instance }})
        expr: (rate(successful_backend_sync_count{type="Pull"}[5m]) < 0.05 or absent(successful_backend_sync_count{type="Pull"}))
        for: 10m
        labels:
          severity: critical
      - alert: RunaiAgentClusterInfoPushRateLow
        annotations:
          summary: Runai cluster-sync fails pushing cluster info the backend (instance
            {{ $labels.instance }})
        expr: (rate(successful_backend_sync_count{type="ClusterInfoPush"}[5m]) < 0.05
          or absent(successful_backend_sync_count{type="ClusterInfoPush"}))
        for: 10m
        labels:
          severity: critical
      - alert: RunaiClusterSyncHandlingRateLow
        annotations:
          summary: Runai cluster-sync has a low object handling rate (for objects
            of type {{ $labels.name }})
        expr: sum(increase(workqueue_depth{container="cluster-sync"}[1m]) > 50.00)
          by (instance, container, name)
        for: 10m
        labels:
          severity: warning
      - alert: RunaiDeploymentInsufficientReplicas
        annotations:
          summary: Runai Deployment has not matched the expected number of replicas.
        expr: sum(min_over_time(kube_deployment_status_replicas_available{namespace=~"runai|runai-backend"}[30s]))
          < sum(min_over_time(kube_deployment_spec_replicas{namespace=~"runai|runai-backend"}[30s]))
        for: 5m
        labels:
          severity: critical
      - alert: RunaiProjectControllerReconcileFailure
        annotations:
          summary: Runai project controller service had a runtime error while reconciling
        expr: increase(controller_runtime_reconcile_errors_total{controller="project"}[10m])
          > 0
        labels:
          severity: critical
      - alert: RunaiStatefulSetInsufficientReplicas
        annotations:
          summary: Runai StatefulSet has not matched the expected number of replicas.
        expr: sum(min_over_time(kube_statefulset_status_replicas_ready{namespace=~"runai|runai-backend"}[30s]))
          < sum(min_over_time(kube_statefulset_status_replicas{namespace=~"runai|runai-backend"}[30s]))
        for: 5m
        labels:
          severity: critical
      - alert: RunaiCriticalProblem
        annotations:
          summary: Runai platform has a critical problem
        expr: sum(ALERTS{alertname=~"Runai.*", severity="critical"} unless ALERTS{alertname="RunaiCriticalProblem"})>
          0
        for: 5m
        labels:
          severity: critical
      - alert: RunaiDaemonSetRolloutStuck
        annotations:
          description: Runai DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} has not finished or progressed for at least 5 minutes.
          summary: Runai DaemonSet rollout is stuck.
        expr: ((kube_daemonset_status_current_number_scheduled{namespace=~"runai|runai-backend"}!=kube_daemonset_status_desired_number_scheduled{namespace=~"runai|runai-backend"})
          or (kube_daemonset_status_number_misscheduled{namespace=~"runai|runai-backend"}!=0)
          or (kube_daemonset_status_updated_number_scheduled{namespace=~"runai|runai-backend"}!=kube_daemonset_status_desired_number_scheduled{namespace=~"runai|runai-backend"})
          or (kube_daemonset_status_number_available{namespace=~"runai|runai-backend"}!=kube_daemonset_status_desired_number_scheduled{namespace=~"runai|runai-backend"}))
        for: 5m
        labels:
          severity: critical
      - alert: RunaiDeploymentUnavailableReplicas
        annotations:
          description: Runai Deployment {{ $labels.namespace }}/{{ $labels.deployment
            }} has unavailable replicas.
          summary: Runai Deployment has unavailable replicas.
        expr: kube_deployment_status_replicas_unavailable{namespace=~"runai|runai-backend"}
          > 0
        for: 5m
        labels:
          severity: warning
      - alert: RunaiDeploymentNoAvailableReplicas
        annotations:
          description: Runai Deployment {{ $labels.namespace }}/{{ $labels.deployment
            }} has no available replicas.
          summary: Runai Deployment has no available replicas.
        expr: (kube_deployment_status_replicas_available{namespace=~"runai|runai-backend"}==0)
          and (kube_deployment_spec_replicas{namespace=~"runai|runai-backend"}>0)
        for: 5m
        labels:
          severity: critical
      - alert: RunaiDaemonSetUnavailableOnNodes
        annotations:
          description: Runai Daemonset {{ $labels.namespace }}/{{ $labels.daemonset
            }} is unavailable on certain nodes.
          summary: Runai Daemonset is unavailable on nodes.
        expr: kube_daemonset_status_number_unavailable{namespace=~"runai|runai-backend"}
          > 0
        for: 5m
        labels:
          severity: critical
      - alert: RunaiStatefulSetNoAvailableReplicas
        annotations:
          description: Runai StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset
            }} has no available replicas.
          summary: Runai StatefulSet has no available replicas.
        expr: (kube_statefulset_status_replicas_available{namespace=~"runai|runai-backend"}==0)
          and (kube_statefulset_replicas{namespace=~"runai|runai-backend"}>0)
        for: 5m
        labels:
          severity: critical
      - alert: RunaiContainerRestarting
        annotations:
          description: Runai container {{ $labels.namespace }}/{{ $labels.pod }}/{{
            $labels.container }} has restarted more than two times in the last 10
            min.
          summary: Runai container is restarting.
        expr: delta(kube_pod_container_status_restarts_total{namespace=~"runai|runai-backend"}[10m])
          >= 2
        for: 10s
        labels:
          severity: warning
      - alert: RunaiContainerMemoryUsageWarning
        annotations:
          description: Runai container {{ $labels.namespace }}/{{ $labels.pod }}/{{
            $labels.container }} is using more than 80% of its memory limit.
          summary: Runai container is using more than 80% of its memory limit.
        expr: ((container_memory_usage_bytes{namespace=~"runai|runai-backend"}>0)
          / on(container, pod, namespace) (kube_pod_container_resource_limits{namespace=~"runai|runai-backend",
          unit='byte'}>0)) > 0.8
        for: 2m
        labels:
          severity: warning
      - alert: RunaiContainerMemoryUsageCritical
        annotations:
          description: Runai container {{ $labels.namespace }}/{{ $labels.pod }}/{{
            $labels.container }} is using more than 90% of its memory limit.
          summary: Runai container is using more than 90% of its memory limit.
        expr: ((container_memory_usage_bytes{namespace=~"runai|runai-backend"}>0)
          / on(container, pod, namespace) (kube_pod_container_resource_limits{namespace=~"runai|runai-backend",
          unit="byte"}>0)) > 0.9
        for: 30s
        labels:
          severity: critical
      - alert: RunaiCpuUsageWarning
        annotations:
          description: Runai container {{ $labels.namespace }}/{{ $labels.pod }}/{{
            $labels.container }} is using more than 80% of its CPU limit.
          summary: Runai container is using more than 80% of its CPU limit.
        expr: ((rate(container_cpu_usage_seconds_total{namespace=~"runai|runai-backend"}[2m])>0)
          / on(container, pod, namespace) (kube_pod_container_resource_limits{namespace=~"runai|runai-backend",
          unit="core"}>0)) > 0.8
        for: 2m
        labels:
          severity: warning
      - alert: RunaiNodeMemoryUsageCritical
        annotations:
          description: 'Node {{ $labels.node }} is using more than 90% of its memory.
            Current memory usage: {{ printf "%.2f" $value }}%'
          summary: Node memory usage is critically high
        expr: runai_used_memory_bytes_per_node / runai_total_memory_bytes_per_node
          * 100 > 90
        for: 30s
        labels:
          severity: critical
      - alert: RunaiNodeMemoryUsageWarning
        annotations:
          description: 'Node {{ $labels.node }} is using more than 80% of its memory.
            Current memory usage: {{ printf "%.2f" $value }}%'
          summary: Node memory usage is high
        expr: runai_used_memory_bytes_per_node / runai_total_memory_bytes_per_node
          * 100 > 80
        for: 2m
        labels:
          severity: warning
      - alert: RunaiNodeUnschedulableOrNotReady
        annotations:
          description: |-
            The node {{ $labels.node }} is in one of the following states:
             1) `Ready=Unknown`: The control plane cannot communicate with the node.
             2) `Ready=False`: The node is not healthy.
             3)`Unschedulable=True`: The node is marked as unschedulable.
          summary: Node {{ $labels.node }} is either unschedulable or has unknown
            status
        expr: runai_node_info_enriched{ready=~"false|unknown"} or runai_node_info_enriched{schedulable="false"}
        for: 2m
        labels:
          severity: critical
kind: List
metadata:
  resourceVersion: ""
